import{t as ae,s as We,d as j,l as ke,a as be,b as ee,c as xe,e as Se,E as ie,m as Re,o as ce,z as ue,f as L,g as qe,w as _e,h as ze,i as Me,j as X,k as ve,n as Ge,p as je,q as Te,r as Pe,u as de,v as Qe,x as oe,y as He,N as Xe}from"./vendor.5623a205.js";const Ue=function(){const e=document.createElement("link").relList;if(e&&e.supports&&e.supports("modulepreload"))return;for(const d of document.querySelectorAll('link[rel="modulepreload"]'))l(d);new MutationObserver(d=>{for(const t of d)if(t.type==="childList")for(const i of t.addedNodes)i.tagName==="LINK"&&i.rel==="modulepreload"&&l(i)}).observe(document,{childList:!0,subtree:!0});function n(d){const t={};return d.integrity&&(t.integrity=d.integrity),d.referrerpolicy&&(t.referrerPolicy=d.referrerpolicy),d.crossorigin==="use-credentials"?t.credentials="include":d.crossorigin==="anonymous"?t.credentials="omit":t.credentials="same-origin",t}function l(d){if(d.ep)return;d.ep=!0;const t=n(d);fetch(d.href,t)}};Ue();const Ze={batchSize:1,numOfChan:1,isColorEnable:!0,isAutoColors:!0,bgLabelValue:0,drawBoundingVolume:!1,isGPU:!0,isBrainCropMaskBased:!0,showPhase1Output:!1,isPostProcessEnable:!0,isContoursViewEnable:!1,browserArrayBufferMaxZDim:30,telemetryFlag:!1,chartXaxisStepPercent:10,uiSampleName:"BC_UI_Sample",atlasSelectedColorTable:"Fire"},D=[{id:1,type:"Segmentation",path:"/models/model5_gw_ae/model.json",modelName:"\u26A1 Tissue GWM (light)",colormapPath:"./models/model5_gw_ae/colormap3.json",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:0,enableTranspose:!0,enableCrop:!0,cropPadding:18,autoThreshold:0,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!1,textureSize:0,warning:null,inferenceDelay:100,description:"Gray and white matter segmentation model. Operates on full T1 image in a single pass, but uses only 5 filters per layer. Can work on integrated graphics cards but is barely large enough to provide good accuracy. Still more accurate than the subvolume model."},{id:2,type:"Segmentation",path:"/models/model20chan3cls/model.json",modelName:"\u{1F52A} Tissue GWM (High Acc)",colormapPath:"./models/model20chan3cls/colormap.json",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:0,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:.2,enableQuantileNorm:!0,filterOutWithPreMask:!1,enableSeqConv:!1,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"Gray and white matter segmentation model. Operates on full T1 image in a single pass but needs a dedicated graphics card to operate. Provides the best accuracy with hard cropping for better speed"},{id:3,type:"Segmentation",path:"/models/model20chan3cls/model.json",modelName:"\u{1F52A} Tissue GWM (High Acc, Low Mem)",colormapPath:"./models/model20chan3cls/colormap.json",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:0,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:.2,enableQuantileNorm:!0,filterOutWithPreMask:!1,enableSeqConv:!0,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"Gray and white matter segmentation model. Operates on full T1 image in a single pass but needs a dedicated graphics card to operate. Provides high accuracy and fit low memory available but slower"},{id:4,type:"Atlas",path:"/models/model30chan18cls/model.json",modelName:"\u{1FA93} Subcortical + GWM (High Mem, Fast)",colormapPath:"./models/model30chan18cls/colormap.json",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:200,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:.2,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!1,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"Parcellation of the brain into 17 regions: gray and white matter plus subcortical areas. This is a robust model able to handle range of data quality, including varying saturation, and even clinical scans. It may work on infant brains, but your mileage may vary."},{id:5,type:"Atlas",path:"/models/model30chan18cls/model.json",modelName:"\u{1FA93} Subcortical + GWM (Low Mem, Slow)",colormapPath:"./models/model30chan18cls/colormap.json",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:200,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:.2,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!0,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"Parcellation of the brain into 17 regions: gray and white matter plus subcortical areas. This is a robust model able to handle range of data quality, including varying saturation, and even clinical scans. It may work on infant brains, but your mileage may vary."},{id:6,type:"Atlas",path:"/models/model18cls/model.json",modelName:"\u{1FA93} Subcortical + GWM (Low Mem, Faster)",colormapPath:"./models/model18cls/colormap.json",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:200,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:.2,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!0,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"Parcellation of the brain into 17 regions: gray and white matter plus subcortical areas. This is a robust model able to handle range of data quality, including varying saturation, and even clinical scans. It may work on infant brains, but your mileage may vary."},{id:7,type:"Atlas",path:"/models/model30chan18cls/model.json",modelName:"\u{1F52A}\u{1FA93} Subcortical + GWM (Failsafe, Less Acc)",colormapPath:"./models/model30chan18cls/colormap.json",preModelId:1,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:200,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:0,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!1,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"Parcellation of the brain into 17 regions: gray and white matter plus subcortical areas. This is not a robust model, it may work on low data quality, including varying saturation, and even clinical scans. It may work also on infant brains, but your mileage may vary."},{id:8,type:"Atlas",path:"/models/model30chan50cls/model.json",modelName:"\u{1F52A} Aparc+Aseg 50 (High Mem, Fast)",colormapPath:"./models/model30chan50cls/colormap.json",preModelId:1,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:200,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:0,enableQuantileNorm:!0,filterOutWithPreMask:!1,enableSeqConv:!1,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"This is a 50-class model, that segments the brain into the Aparc+Aseg Freesurfer Atlas but one where cortical homologues are merged into a single class."},{id:9,type:"Atlas",path:"/models/model30chan50cls/model.json",modelName:"\u{1F52A} Aparc+Aseg 50 (Low Mem, Slow)",colormapPath:"./models/model30chan50cls/colormap.json",preModelId:1,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:200,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:0,enableQuantileNorm:!0,filterOutWithPreMask:!1,enableSeqConv:!0,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"This is a 50-class model, that segments the brain into the Aparc+Aseg Freesurfer Atlas but one where cortical homologues are merged into a single class. The model use sequential convolution for inference to overcome browser memory limitations but leads to longer computation time."},{id:10,type:"Brain_Extraction",path:"/models/model5_gw_ae/model.json",modelName:"\u26A1 Extract the Brain (FAST)",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:0,enableTranspose:!0,enableCrop:!0,cropPadding:18,autoThreshold:0,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!1,textureSize:0,warning:null,inferenceDelay:100,description:"Extract the brain fast model operates on full T1 image in a single pass, but uses only 5 filters per layer. Can work on integrated graphics cards but is barely large enough to provide good accuracy. Still more accurate than the failsafe version."},{id:11,type:"Brain_Extraction",path:"/models/model11_gw_ae/model.json",modelName:"\u{1F52A} Extract the Brain (High Acc, Slow)",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:0,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:0,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!0,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"Extract the brain high accuracy model operates on full T1 image in a single pass, but uses only 11 filters per layer. Can work on dedicated graphics cards. Still more accurate than the fast version."},{id:12,type:"Brain_Masking",path:"/models/model5_gw_ae/model.json",modelName:"\u26A1 Brain Mask (FAST)",colormapPath:"./models/model5_gw_ae/colormap.json",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:0,enableTranspose:!0,enableCrop:!0,cropPadding:17,autoThreshold:0,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!1,textureSize:0,warning:null,inferenceDelay:100,description:"This fast masking model operates on full T1 image in a single pass, but uses only 5 filters per layer. Can work on integrated graphics cards but is barely large enough to provide good accuracy. Still more accurate than failsafe version."},{id:13,type:"Brain_Masking",path:"/models/model11_gw_ae/model.json",modelName:"\u{1F52A} Brain Mask (High Acc, Low Mem)",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:0,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:0,enableQuantileNorm:!0,filterOutWithPreMask:!1,enableSeqConv:!0,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"This masking model operates on full T1 image in a single pass, but uses 11 filters per layer. Can work on dedicated graphics cards. Still more accurate than fast version."},{id:14,type:"Atlas",path:"/models/model21_104class/model.json",modelName:"\u{1F52A} Aparc+Aseg 104 (High Mem, Fast)",colormapPath:"./models/model21_104class/colormap.json",preModelId:1,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:200,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:0,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!1,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"FreeSurfer aparc+aseg atlas 104 parcellate brain areas into 104 regions. It contains a combination of the Desikan-Killiany atlas for cortical area and also segmentation of subcortical regions."},{id:15,type:"Atlas",path:"/models/model21_104class/model.json",modelName:"\u{1F52A} Aparc+Aseg 104 (Low Mem, Slow)",colormapPath:"./models/model21_104class/colormap.json",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:200,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:0,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!0,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"FreeSurfer aparc+aseg atlas 104 parcellate brain areas into 104 regions. It contains a combination of the Desikan-Killiany atlas for cortical area and also segmentation of subcortical regions. The model use sequential convolution for inference to overcome browser memory limitations but leads to longer computation time. "}];class Ke{idx(e,n,l,d){return l*d[0]*d[1]+n*d[0]+e}check_previous_slice(e,n,l,d,t,i,a,f,c,o){let s=0;if(!t)return 0;const h=e[this.idx(l,d,t,i)];if(a>=6){const p=this.idx(l,d,t-1,i);h===e[p]&&(c[s++]=n[p])}if(a>=18){if(l){const p=this.idx(l-1,d,t-1,i);h===e[p]&&(c[s++]=n[p])}if(d){const p=this.idx(l,d-1,t-1,i);h===e[p]&&(c[s++]=n[p])}if(l<i[0]-1){const p=this.idx(l+1,d,t-1,i);h===e[p]&&(c[s++]=n[p])}if(d<i[1]-1){const p=this.idx(l,d+1,t-1,i);h===e[p]&&(c[s++]=n[p])}}if(a===26){if(l&&d){const p=this.idx(l-1,d-1,t-1,i);h===e[p]&&(c[s++]=n[p])}if(l<i[0]-1&&d){const p=this.idx(l+1,d-1,t-1,i);h===e[p]&&(c[s++]=n[p])}if(l&&d<i[1]-1){const p=this.idx(l-1,d+1,t-1,i);h===e[p]&&(c[s++]=n[p])}if(l<i[0]-1&&d<i[1]-1){const p=this.idx(l+1,d+1,t-1,i);h===e[p]&&(c[s++]=n[p])}}return s?(this.fill_tratab(f,c,s,o),c[0]):0}do_initial_labelling(e,n,l){const d=new Uint32Array(32),t=new Uint32Array(32);let i=1;const a=8192;let f=a,c=new Uint32Array(f).fill(0);const o=new Uint32Array(n[0]*n[1]*n[2]).fill(0),s=new Uint32Array(27);for(let h=0;h<n[2];h++)for(let p=0;p<n[1];p++)for(let u=0;u<n[0];u++){let m=0;const y=e[this.idx(u,p,h,n)];if(y!==0){if(s[0]=this.check_previous_slice(e,o,u,p,h,n,l,c,d,t),s[0]&&(m+=1),l>=6){if(u){const g=this.idx(u-1,p,h,n);y===e[g]&&(s[m++]=o[g])}if(p){const g=this.idx(u,p-1,h,n);y===e[g]&&(s[m++]=o[g])}}if(l>=18){if(p&&u){const g=this.idx(u-1,p-1,h,n);y===e[g]&&(s[m++]=o[g])}if(p&&u<n[0]-1){const g=this.idx(u+1,p-1,h,n);y===e[g]&&(s[m++]=o[g])}}if(m)o[this.idx(u,p,h,n)]=s[0],this.fill_tratab(c,s,m,t);else{if(o[this.idx(u,p,h,n)]=i,i>=f){f+=a;const g=new Uint32Array(f);g.set(c),c=g}c[i-1]=i,i++}}}for(let h=0;h<i-1;h++){let p=h;for(;c[p]!==p+1;)p=c[p]-1;c[h]=p+1}return[i-1,c,o]}fill_tratab(e,n,l,d){let i=2147483647;for(let a=0;a<l;a++){let f=n[a];for(;e[f-1]!==f;)f=e[f-1];d[a]=f,i=Math.min(i,f)}for(let a=0;a<l;a++)e[d[a]-1]=i}translate_labels(e,n,l,d){const t=n[0]*n[1]*n[2];let i=0;const a=new Uint32Array(t).fill(0);for(let o=0;o<d;o++)i=Math.max(i,l[o]);const f=new Uint32Array(i).fill(0);let c=0;for(let o=0;o<t;o++)e[o]&&(f[l[e[o]-1]-1]||(c+=1,f[l[e[o]-1]-1]=c),a[o]=f[l[e[o]-1]-1]);return[c,a]}largest_original_cluster_labels(e,n,l){const d=e.length,t=new Uint32Array(n+1).fill(0),i=new Uint32Array(n+1).fill(0);for(let c=0;c<d;c++){const o=e[c],s=l[c];t[s]=o,i[s]++}let a=0;for(let c=0;c<n+1;c++){const o=t[c];a=Math.max(a,o);for(let s=0;s<n+1;s++)s!==c&&o===t[s]&&(i[c]<i[s]||i[c]===i[s]&&c<s)&&(t[c]=0)}const f=new Uint32Array(d).fill(0);for(let c=0;c<d;c++)f[c]=t[l[c]];return[a,f]}bwlabel(e,n,l=26,d=!1,t=!1){const i=Date.now(),a=n[0]*n[1]*n[2],f=new Uint32Array(a).fill(0);if(![6,18,26].includes(l))return console.log("bwlabel: conn must be 6, 18 or 26."),[0,f];if(n[0]<2||n[1]<2||n[2]<1)return console.log("bwlabel: img must be 2 or 3-dimensional"),[0,f];if(d)for(let u=0;u<a;u++)e[u]!==0&&(f[u]=1);else f.set(e);let[c,o,s]=this.do_initial_labelling(f,n,l);o===void 0&&(o=new Uint32Array(0));const[h,p]=this.translate_labels(s,n,o,c);if(console.log(l+" neighbor clustering into "+h+" regions in "+(Date.now()-i)+"ms"),t){const[u,m]=this.largest_original_cluster_labels(f,h,p);return[u,m]}return[h,p]}}async function Ce(r,e=[1,1],n=[1,1],l=[1,1]){if(r.rank!==3)throw new Error("Tensor must be 3D");return r.pad([e,n,l])}async function Le(r,e){const n=r.max(),l=n.mul(e),d=await l.data();return n.dispose(),l.dispose(),ee(()=>r.clone().greater(d[0]))}async function pe(r){const e=0;return r.step(e)}async function Je(r,e=.01,n=.99){const l=r.flatten(),d=await l.array();d.sort((p,u)=>p-u);const t=xe(d),i=t.shape[0],a=Math.floor(i*e),f=Math.ceil(i*n)-1,c=t.slice(a,1),o=t.slice(f,1),s=(await c.array())[0],h=(await o.array())[0];return l.dispose(),t.dispose(),c.dispose(),o.dispose(),{qmin:s,qmax:h}}async function $e(r,e,n,l,d,t,i){const a=r.shape[4],f=e.shape[4];let c=null;for(let o=0;o<f;o++){const s=Math.ceil(a/i),h=n.slice([o],[1]);let p=null;for(let m=0;m<s;m++){const y=m*i,g=Math.min((m+1)*i,a);if(y<a){const S=ee(()=>{const b=r.slice([0,0,0,0,y],[-1,-1,-1,-1,g-y]),I=e.slice([0,0,0,y,o],[-1,-1,-1,g-y,1]);return Se(b,I,l,d,"NDHWC",t)});if(p===null)p=S;else{const b=p.add(S);p.dispose(),S.dispose(),p=b}}}const u=p.add(h);if(p.dispose(),h.dispose(),c==null)c=u;else{const m=await Ge([c,u],4);u.dispose(),c.dispose(),c=m}}return c}async function Oe(r,e,n,l){const d=[];for(let c=0;c<r.length;c++)d[c]=Array.from(r[c].dataSync());const t=new Array(d[0].length*d.length);let i=0;for(let c=0;c<d.length;c++)for(let o=0;o<d[c].length;o++)t[i++]=d[c][o];console.log("Done with allOutputSlices3DCC1DimArray ");const a=await pe(xe(t)),f=Array.from(a.dataSync());l(f,e,n)}async function fe(r,e=0){let n=[];e===0?n=await r.max(2).max(1).arraySync():e===1?n=await r.max(2).max(0).arraySync():n=await r.max(1).max(0).arraySync();let l=n.length,d=0;for(let t=0;t<n.length;t++)if(n[t]>0){l=t;break}for(let t=n.length-1;t>=0;t--)if(n[t]>0){d=t;break}return[l,d]}async function Ie(r){const[e,n]=await fe(r,0),[l,d]=await fe(r,1),[t,i]=await fe(r,2);return console.log("row min and max  :",e,n),console.log("col min and max  :",l,d),console.log("depth min and max  :",t,i),[e,n,l,d,t,i]}async function Ye(r,e,n,l,d,t,i,a,f=!0){r[0].dtype!=="int32"&&i("",-1,"generateBrainMask assumes int32"),d.preModelPostProcess&&i("",-1,"generateBrainMask assumes BWLabeler instead of preModelPostProcess");const c=r.length,o=r[0].size,s=c*o,h=new Int32Array(s);let p=0;for(let u=0;u<c;u++)h.set(r[u].dataSync(),p),p+=o;for(let u=0;u<s;u++)h[u]=h[u]!==0?1:0;return(f||t.showPhase1Output)&&(a(h,t,d),i("Segmentation finished",0)),ae(h,[e,n,l])}async function Ae(r,e,n,l,d,t,i,a,f,c){if(f.isPostProcessEnable){const s=new Ke,h=new Uint32Array(e),p=26,u=!0,m=!0,[y,g]=s.bwlabel(r,h,p,u,m);for(let S=0;S<r.length;S++)r[S]*=g[S]}const o=new Uint8Array(r);switch(a.type){case"Brain_Masking":{const s=new Uint8Array(o.length);for(let h=0;h<o.length;h++)s[h]=o[h]!==0?1:0;return s}case"Brain_Extraction":{const s=new Uint8Array(o.length);for(let h=0;h<o.length;h++){const p=o[h]!==0?1:0;s[h]=c[h]*p}return s}}return r}async function De(r,e,n){const l=e.dims[1],d=e.dims[2];let t;if(e.datatypeCode===2)t=new Uint8Array(n);else if(e.datatypeCode===4)t=new Int16Array(n);else if(e.datatypeCode===8)t=new Int32Array(n);else if(e.datatypeCode===16)t=new Float32Array(n);else if(e.datatypeCode===64)t=new Float64Array(n);else if(e.datatypeCode===256)t=new Int8Array(n);else if(e.datatypeCode===512)t=new Uint16Array(n);else if(e.datatypeCode===768)t=new Uint32Array(n);else return;const i=[];let a=0;for(let c=0;c<r;c++){const o=new Array(d*l);let s=0;for(let h=0;h<d;h++)for(let p=0;p<l;p++){const u=t[a++];o[s++]=u&255}i.push(ae(o,[d,l]))}const f=We(i);return j(i),f}async function he(r){return r.layers.length}async function me(r){let e=0;for(let n=0;n<r.layers.length;n++)e+=r.layers[n].countParams();return e}async function re(r){for(let e=0;e<r.layers.length;e++)if(r.layersByDepth[e][0].dataFormat)return r.layersByDepth[e][0].dataFormat==="channelsLast"}async function Ee(r){return await ke(r)}async function ge(r){const e=r.max(),n=r.min();return await r.sub(n).div(e.sub(n))}function eo(r,e,n){const l=1,d=0,t=1,i=r.shape[4],a=Math.ceil(i/n);let f=null;for(let c=0;c<a;c++){const o=c*n,h=Math.min((c+1)*n,i)-o,p=ee(()=>r.slice([0,0,0,0,o],[-1,-1,-1,-1,h])),u=ee(()=>e.slice([0,0,0,o,0],[-1,-1,-1,h,-1])),m=Se(p,u,l,d,"NDHWC",t);p.dispose(),u.dispose();const y=je(m);if(m.dispose(),f===null)f=y;else{const g=f.add(y);f.dispose(),f!==y&&y.dispose(),f=g}ee(()=>{Me(ue([1,1]),ue([1,1]))})}return f}async function ye(r,e=.05,n=.95){const{qmin:l,qmax:d}=await Je(r,e,n),t=be(l),i=be(d),a=r.sub(t).div(i.sub(t));return t.dispose(),i.dispose(),a}async function se(r,e=1,n=1,l=1){if(r.rank!==3)throw new Error("Tensor must be 3D");const[d,t,i]=r.shape;return r.slice([e,n,l],[d-2*e,t-2*n,i-2*l])}async function te(r,e,n,l,d,t){const i=d[0],a=d[1],f=d[2],c=i+t[0]-1,o=a+t[1]-1,s=f+t[2]-1,h=n-c-1>0?n-c-1:0,p=l-o-1>0?l-o-1:0,u=e-s-1>0?e-s-1:0;return r.pad([[i,h],[a,p],[f,u]])}class oo{constructor(e,n,l,d,t=!0){this.model=e,this.outChannels=e.outputLayers[0].kernel.shape[4],this.chunkSize=n,this.isChannelLast=l,this.callbackUI=d,this.isWebWorker=t}async apply(e){const n=ie.get("WEBGL_DELETE_TEXTURE_THRESHOLD");ie.set("WEBGL_DELETE_TEXTURE_THRESHOLD",0);const l=this,d=performance.now(),t=l.model.layers[l.model.layers.length-1],i=t.getWeights()[0],a=t.getWeights()[1],f=l.isChannelLast?e.shape.slice(1,-1):e.shape.slice(2);let c=Re(ce(f),-1e4),o=ue(f),s=0;for(console.log(" channel loop");;){L().startScope();const h=await ee(()=>{const u=i.slice([0,0,0,0,s],[-1,-1,-1,-1,1]),m=a.slice([s],[1]),y=eo(e,u,Math.min(l.chunkSize,l.outChannels)).add(m),g=qe(y,c),S=_e(g,y,c),b=_e(g,ze(o.shape,s),o);return j([c,o,u,m,y,g]),ee(()=>Me(ce([1,1]),ce([1,1]))),[b,S]});console.log("======================="),l.callbackUI(`Iteration ${s}`,s/l.outChannels),l.isWebWorker||await new Promise(u=>setTimeout(u,17));const p=await X();if(console.log(`Number of Tensors: ${p.numTensors}`),console.log(`Number of Data Buffers: ${p.numDataBuffers}`),console.log(`Megabytes In Use: ${(p.numBytes/1048576).toFixed(3)} MB`),p.unreliable&&console.log(`Unreliable: ${p.unreliable}`),typeof o!="undefined"&&o.dispose(),typeof c!="undefined"&&c.dispose(),o=ve(h[0]),c=ve(h[1]),L().endScope(),s===l.outChannels-1){j(c);const m=performance.now()-d;return console.log(`Execution time for output layer: ${m} milliseconds`),ie.set("WEBGL_DELETE_TEXTURE_THRESHOLD",n),o}else{s++;const u=o.shape,m=o.dataSync(),y=o.shape,g=c.dataSync();o.dispose(),c.dispose(),o=ae(m,u),c=ae(g,y)}}}}async function Be(r,e,n,l,d,t,i,a,f,c,o,s){console.log(" ---- Start FullVolume Inference with Sequential Conv Layer for phase-II ---- "),e.enableQuantileNorm?(console.log("preModel Quantile normalization enabled"),l=await ye(l)):(console.log("preModel Min Max normalization enabled"),l=await ge(l));let p;if(a==null){const O=e.autoThreshold;O>0&&O<=1?p=await Le(l,O):(console.log("No valid crop threshold value"),p=await l.greater([0]).asType("bool"))}else p=await a.greater([0]).asType("bool");console.log(" mask_3d shape :  ",p.shape);const[u,m,y,g,S,b]=await Ie(p);p.dispose();const I=[u,y,S],R=[m-u+1,g-y+1,b-S+1],U=await l.slice([u,y,S],[m-u+1,g-y+1,b-S+1]);l.dispose();const _=e.cropPadding;let T=await Ce(U,[_,_],[_,_],[_,_]);if(console.log(" cropped slices_3d with padding shape:  ",T.shape),U.dispose(),r.drawBoundingVolume){let O=await se(T,_,_,_);return console.log(" outLabelVolume without padding shape :  ",O.shape),O=await te(O,d,t,i,I,R),console.log(" outLabelVolume final shape after resizing :  ",O.shape),Oe(de(O),r,e,c),O.dispose(),0}o.Brainchop_Ver="FullVolume";const w=await n;try{let O=performance.now();const Q=performance.now();let C=0;const v=e.enableTranspose,J=e.inferenceDelay;console.log("Inference delay :",J),v?(T=await T.transpose(),console.log("Input transposed for pre-model")):console.log("Transpose not enabled for pre-model");let M=1;const Z=w.layers.length;console.log("res.layers.length ",Z);const x=re(w),H=r.batchSize,W=r.numOfChan;let $;x?(w.layers[0].batchInputShape[1]=T.shape[0],w.layers[0].batchInputShape[2]=T.shape[1],w.layers[0].batchInputShape[3]=T.shape[2],$=[H,w.layers[0].batchInputShape[1],w.layers[0].batchInputShape[2],w.layers[0].batchInputShape[3],W]):(w.layers[0].batchInputShape[2]=T.shape[0],w.layers[0].batchInputShape[3]=T.shape[1],w.layers[0].batchInputShape[4]=T.shape[2],$=[H,W,w.layers[0].batchInputShape[2],w.layers[0].batchInputShape[3],w.layers[0].batchInputShape[4]]),console.log(" Model batch input shape : ",w.layers[0].batchInputShape),o.Input_Shape=JSON.stringify(w.layers[0].batchInputShape),o.Output_Shape=JSON.stringify(w.output.shape),o.Channel_Last=x,o.Model_Param=await me(w),o.Model_Layers=await he(w),o.Model=e.modelName,o.Seq_Conv=e.enableSeqConv,o.Extra_Info=null;const K=w.layers[w.layers.length-1];console.log("Output Layer : ",K);const B=x?K.outputShape[K.outputShape.length-1]:K.outputShape[1];console.log("Num of output channels : ",B);const E=[];E[0]=await T.reshape($);const q=window.setInterval(async function(){try{w.layers[M].activation.getClassName()!=="linear"?E[M]=await w.layers[M].apply(E[M-1]):E[M]=await $e(E[M-1],w.layers[M].getWeights()[0],w.layers[M].getWeights()[1],w.layers[M].strides,w.layers[M].padding,w.layers[M].dilationRate,3),j(E[M-1])}catch(F){const N="Your graphics card (e.g. Intel) may not be compatible with WebGL. "+F.message;return f(N,-1,N),window.clearInterval(q),L().endScope(),L().disposeVariables(),o.Inference_t=1/0,o.Postprocess_t=1/0,o.Status="Fail",o.Error_Type=F.message,o.Extra_Err_Info="Failed while model layer "+M+" apply",f("",-1,"",o),0}if(console.log("layer output Tensor shape : ",E[M].shape),console.log("layer count params ",w.layers[M].countParams()),w.layers[M].dispose(),E[M-1].dispose(),f("Layer "+M.toString(),(M+1)/Z),X().unreliable){const F="unreliable reasons :"+X().reasons;f(F,NaN,F)}if(M===Z-2){window.clearInterval(q);const N=await(await new oo(w,10,x,f,!1)).apply(E[M]);if(f("seqConvLayer Done"),j(E[M]),console.log(" Output tensor",N),console.log(" Output tensor shape : ",N.shape),N.shape.length!==3){const A="Output tensor shape should be 3 dims but it is "+N.shape.length;f(A,-1,A)}const z=((performance.now()-O)/1e3).toFixed(4);console.log(" find array max ");const k=await N.max().dataSync()[0];C<k&&(C=k);const V=C+1;if(console.log("Predicted num of segmentation classes",V),o.Actual_Labels=V,o.Expect_Labels=B,o.NumLabels_Match=V===B,V!==B){const A="expected "+B+" labels, but the predicted are "+V;f(A,-1,A)}let P=N.reshape([T.shape[0],T.shape[1],T.shape[2]]);j(N),v&&(console.log("outLabelVolume transposed"),P=P.transpose()),P=await se(P,_,_,_),console.log(" outLabelVolume without padding shape :  ",P.shape),P=await te(P,d,t,i,I,R),console.log(" outLabelVolume final shape after resizing :  ",P.shape);const le=e.filterOutWithPreMask;if(a!=null&&r.isBrainCropMaskBased&&le){const A=await pe(a);P=await P.mul(A)}O=performance.now(),console.log("Generating correct output");let ne;try{const A=await new Uint32Array(P.dataSync()),Y=P.shape,Ve=P.dtype;ne=await Ae(A,Y,Ve,d,V,t,i,e,r,s),console.log(" Phase-2 num of tensors after generateOutputSlicesV2: ",X().numTensors),j(P),L().endScope(),L().disposeVariables()}catch(A){L().endScope(),L().disposeVariables(),console.log("Error while generating output: ",A);const Y="Failed while generating output due to limited browser memory available";return f(Y,-1,Y),o.Inference_t=z,o.Postprocess_t=1/0,o.Status="Fail",o.Error_Type=A.message,o.Extra_Err_Info="Failed while generating output",f("",-1,"",o),0}const G=((performance.now()-O)/1e3).toFixed(4);return console.log("Processing the whole brain volume in tfjs for multi-class output mask took : ",((performance.now()-Q)/1e3).toFixed(4)+"  Seconds"),o.Inference_t=z,o.Postprocess_t=G,o.Status="OK",f("",-1,"",o),f("Segmentation finished",0),c(ne,r,e),0}else M++},J)}catch(O){if(f(O.message,-1,O.message),console.log('If webgl context is lost, try to restore webgl context by visit the link <a href="https://support.biodigital.com/hc/en-us/articles/218322977-How-to-turn-on-WebGL-in-my-browser">here</a>'),X().unreliable){const Q="unreliable reasons :"+X().reasons;f(Q,NaN,Q)}}}async function Ne(r,e,n,l,d,t,i,a,f,c,o,s){let h=[];console.log(" ---- Start FullVolume inference phase-II ---- "),i.enableQuantileNorm?(console.log("preModel Quantile normalization enabled"),e=await ye(e)):(console.log("preModel Min Max normalization enabled"),e=await ge(e));let u;if(t==null){const v=i.autoThreshold;v>0&&v<=1?u=await Le(e,v):(console.log("No valid crop threshold value"),u=await e.greater([0]).asType("bool"))}else u=t.greater([0]).asType("bool");console.log(" mask_3d shape :  ",u.shape);const[m,y,g,S,b,I]=await Ie(u);u.dispose();const R=[m,g,b];console.log("refVoxel :",R);const U=[y-m+1,S-g+1,I-b+1];console.log("boundVolSizeArr :",U);const _=e.slice([m,g,b],[y-m+1,S-g+1,I-b+1]);e.dispose();const T=i.cropPadding;let w=await Ce(_,[T,T],[T,T],[T,T]);if(console.log(" cropped slices_3d with padding shape:  ",w.shape),_.dispose(),f.drawBoundingVolume){let v=await se(w,T,T,T);return console.log(" outLabelVolume without padding shape :  ",v.shape),v=await te(v,n,l,d,R,U),console.log(" outLabelVolume final shape after resizing :  ",v.shape),Oe(de(v),f,i,c),v.dispose(),0}a.Brainchop_Ver="FullVolume";let O=performance.now(),Q=[];const C=await r;try{O=performance.now();const v=performance.now();let J=0;const M=i.enableTranspose,Z=i.inferenceDelay;console.log("Inference delay :",Z),M?(w=w.transpose(),console.log("Input transposed for pre-model")):console.log("Transpose not enabled for pre-model");let x=1;const H=C.layers.length;console.log("res.layers.length ",H);const W=await re(C),$=f.batchSize,K=f.numOfChan;W?(C.layers[0].batchInputShape[1]=w.shape[0],C.layers[0].batchInputShape[2]=w.shape[1],C.layers[0].batchInputShape[3]=w.shape[2],Q=[$,C.layers[0].batchInputShape[1],C.layers[0].batchInputShape[2],C.layers[0].batchInputShape[3],K]):(C.layers[0].batchInputShape[2]=w.shape[0],C.layers[0].batchInputShape[3]=w.shape[1],C.layers[0].batchInputShape[4]=w.shape[2],Q=[$,K,C.layers[0].batchInputShape[2],C.layers[0].batchInputShape[3],C.layers[0].batchInputShape[4]]),console.log(" Model batch input shape : ",C.layers[0].batchInputShape),a.Input_Shape=JSON.stringify(C.layers[0].batchInputShape),a.Output_Shape=JSON.stringify(C.output.shape),a.Channel_Last=W,a.Model_Param=await me(C),a.Model_Layers=await he(C),a.Model=i.modelName,a.Extra_Info=null;const B=[];B[0]=w.reshape(Q);const E=window.setInterval(async function(){try{B[x]=C.layers[x].apply(B[x-1])}catch(q){return o(q.message,-1,q.message),window.clearInterval(E),L().endScope(),L().disposeVariables(),a.Inference_t=1/0,a.Postprocess_t=1/0,a.Status="Fail",a.Error_Type=q.message,a.Extra_Err_Info="Failed while model layer "+x+" apply",o("",-1,"",a),0}if(o("Layer "+x.toString(),(x+1)/H),console.log("layer output Tensor shape : ",B[x].shape),console.log("layer count params ",C.layers[x].countParams()),C.layers[x].dispose(),B[x-1].dispose(),X().unreliable){const q="unreliable reasons :"+X().reasons;o(q,NaN,q)}if(x===H-1){window.clearInterval(E);const q=W?-1:1;console.log(" find argmax "),console.log("last Tensor shape : ",B[x].shape);const F=W?B[x].shape[4]:B[x].shape[1];let N;try{const G=performance.now();console.log(" Try tf.argMax for fullVolume .."),N=Pe(B[x],q),console.log("tf.argMax for fullVolume takes : ",((performance.now()-G)/1e3).toFixed(4))}catch(G){if(q===-1)try{const A=performance.now();console.log(" tf.argMax failed .. try argMaxLarge .."),window.alert("tensor2LightBuffer() is not dead code?"),window.alert("argMaxLarge() is not dead code?"),console.log("argMaxLarge for fullVolume takes : ",((performance.now()-A)/1e3).toFixed(4))}catch(A){const Y="argMax buffer couldn't be created due to limited memory resources.";return o(Y,-1,Y),window.clearInterval(E),L().endScope(),L().disposeVariables(),a.Inference_t=1/0,a.Postprocess_t=1/0,a.Status="Fail",a.Error_Type=A.message,a.Extra_Err_Info="prediction_argmax from argMaxLarge failed",o("",-1,"",a),0}else{const A="argMax buffer couldn't be created due to limited memory resources.";return o(A,-1,A),N.dispose(),window.clearInterval(E),L().endScope(),L().disposeVariables(),a.Inference_t=1/0,a.Postprocess_t=1/0,a.Status="Fail",a.Error_Type=G.message,a.Extra_Err_Info="prediction_argmax from argMaxLarge not support yet channel first",o("",-1,"",a),0}}console.log(" prediction_argmax shape : ",N.shape);const z=((performance.now()-O)/1e3).toFixed(4);j(B[x]);const k=await N.max().dataSync()[0];J<k&&(J=k);const V=J+1;if(console.log("numSegClasses",V),a.Actual_Labels=V,a.Expect_Labels=F,a.NumLabels_Match=V===F,V!==F){const G="expected "+F+" labels, but the predicted are "+V;o(G,-1,G)}let P=N.reshape([w.shape[0],w.shape[1],w.shape[2]]);j(N),M&&(console.log("outLabelVolume transposed"),P=P.transpose()),P=await se(P,T,T,T),console.log(" outLabelVolume without padding shape :  ",P.shape),P=await te(P,n,l,d,R,U),console.log(" outLabelVolume final shape after resizing :  ",P.shape);const le=i.filterOutWithPreMask;if(t!=null&&f.isBrainCropMaskBased&&le){const G=pe(t);P=P.mul(G)}O=performance.now(),console.log("Generating correct output");try{const G=new Uint32Array(P.dataSync()),A=P.shape,Y=P.dtype;j(P),L().endScope(),L().disposeVariables(),h=await Ae(G,A,Y,n,V,l,d,i,f,s),console.log(" Phase-2 num of tensors after generateOutputSlicesV2: ",X().numTensors)}catch(G){L().endScope(),L().disposeVariables();const A="Failed while generating output due to limited browser memory available";return o(A,-1,A),a.Inference_t=z,a.Postprocess_t=1/0,a.Status="Fail",a.Error_Type=G.message,a.Extra_Err_Info="Failed while generating output",o("",-1,"",a),0}const ne=((performance.now()-O)/1e3).toFixed(4);return L().disposeVariables(),console.log("Processing the whole brain volume in tfjs for multi-class output mask took : ",((performance.now()-v)/1e3).toFixed(4)+"  Seconds"),a.Inference_t=z,a.Postprocess_t=ne,a.Status="OK",o("",-1,"",a),clearInterval(E),o("Segmentation finished",0),c(h,f,i),0}x++},Z)}catch(v){o(v.message,-1,v.message),console.log('If webgl context is lost, try to restore webgl context by visit the link <a href="https://support.biodigital.com/hc/en-us/articles/218322977-How-to-turn-on-WebGL-in-my-browser">here</a>')}}async function no(r,e,n,l,d,t,i,a,f,c,o,s){if(a.No_SubVolumes=1,i.preModelId){const h=await Ee(f.rootURL+D[i.preModelId-1].path),p=D[i.preModelId-1].enableTranspose,u=D[i.preModelId-1].enableQuantileNorm;let m=null;u?(console.log("preModel Quantile normalization enabled"),m=await ye(e)):(console.log("preModel Min Max normalization enabled"),m=await ge(e)),p?(m=await m.transpose(),console.log("Input transposed for pre-model")):console.log("Transpose not enabled for pre-model"),a.Brainchop_Ver="PreModel_FV";const y=await h;try{const g=performance.now(),S=y,b=S.layers[0].batchInputShape;if(console.log(" Pre-Model batch input shape : ",b),b.length!==5){const x="The pre-model input shape must be 5D ";return o(x,-1,x),0}const I=re(S),R=f.batchSize,U=f.numOfChan;let _,T,w,O;if(I){if(console.log("Pre-Model Channel Last"),isNaN(b[4])||b[4]!==1){const x="The number of channels for pre-model input shape must be 1";return o(x,-1,x),0}_=b[1],T=b[2],w=b[3],O=[R,_,T,w,U]}else{if(console.log("Pre-Model Channel First"),isNaN(b[1])||b[1]!==1){const x="The number of channels for pre-model input shape must be 1";return o(x,-1,x),0}_=b[2],T=b[3],w=b[4],O=[R,U,_,T,w]}a.Input_Shape=JSON.stringify(O),a.Output_Shape=JSON.stringify(S.output.shape),a.Channel_Last=I,a.Model_Param=await me(S),a.Model_Layers=await he(S);let Q=0;const C=D[i.preModelId-1].inferenceDelay;let v=1;const J=y.layers.length,M=[];M[0]=m.reshape(O),j(m);const Z=window.setInterval(async function(){try{M[v]=await y.layers[v].apply(M[v-1])}catch(x){const H="Your graphics card (e.g. Intel) may not be compatible with WebGL. "+x.message;return o(H,-1,H),window.clearInterval(Z),L().endScope(),L().disposeVariables(),a.Inference_t=1/0,a.Postprocess_t=1/0,a.Status="Fail",a.Error_Type=x.message,a.Extra_Err_Info="PreModel Failed while model layer "+v+" apply",o("",-1,"",a),0}if(y.layers[v].dispose(),M[v-1].dispose(),o("Layer "+v.toString(),(v+1)/J),X().unreliable){const x="unreliable reasons :"+X().reasons;o(x,NaN,x)}if(v===J-1){window.clearInterval(Z);const x=I?-1:1;console.log(" find argmax "),console.log("last Tensor shape : ",M[v].shape);const H=I?M[v].shape[4]:M[v].shape[1];let W;try{console.log(" Try tf.argMax for fullVolume .."),W=await Pe(M[v],x)}catch(z){if(x===-1)try{const k=performance.now();console.log(" tf.argMax failed .. try argMaxLarge .."),window.alert("tensor2LightBuffer() is not dead code?"),window.alert("argMaxLarge() is not dead code?"),console.log("argMaxLarge for fullVolume takes : ",((performance.now()-k)/1e3).toFixed(4))}catch(k){const V="argMax buffer couldn't be created due to limited memory resources.";return o(V,-1,V),W.dispose(),window.clearInterval(Z),L().endScope(),L().disposeVariables(),a.Inference_t=1/0,a.Postprocess_t=1/0,a.Status="Fail",a.Error_Type=k.message,a.Extra_Err_Info="preModel prediction_argmax from argMaxLarge failed",o("",-1,"",a),0}else{const k="argMax buffer couldn't be created due to limited memory resources.";return o(k,-1,k),W.dispose(),window.clearInterval(Z),L().endScope(),L().disposeVariables(),a.Inference_t=1/0,a.Postprocess_t=1/0,a.Status="Fail",a.Error_Type=z.message,a.Extra_Err_Info="preModel prediction_argmax from argMaxLarge not support yet channel first",o("",-1,"",a),0}}console.log(" Pre-model prediction_argmax shape : ",W.shape);const $=((performance.now()-g)/1e3).toFixed(4);j(M[v]),console.log(" Pre-model find array max ");const K=await W.max().dataSync()[0];Q<K&&(Q=K);const B=Q+1;console.log("Pre-model numSegClasses",B),a.Actual_Labels=B,a.Expect_Labels=H,a.NumLabels_Match=B===H;let E=await W.reshape([n,l,d]);j(W),p&&(console.log("Pre-model outLabelVolume transposed"),E=E.transpose());const q=performance.now();console.log("Generating pre-model output");let F;try{const z=await de(E);F=await Ye(z,n,l,d,i,f,o,c,!1),await j(E),console.log(" Phase-1 num of tensors after generateBrainMask: ",X().numTensors)}catch(z){L().endScope(),L().disposeVariables();const k="Failed while generating pre-model output due to limited browser memory available";return o(k,-1,k),a.Inference_t=$,a.Postprocess_t=1/0,a.Status="Fail",a.Error_Type=z.message,a.Extra_Err_Info="Pre-model failed while generating output",o("",-1,"",a),0}const N=((performance.now()-q)/1e3).toFixed(4);if(console.log("Pre-model processing the whole brain volume in tfjs tooks for multi-class output mask : ",((performance.now()-g)/1e3).toFixed(4)+"  Seconds"),a.Inference_t=$,a.Postprocess_t=N,a.Status="OK",o("",-1,"",a),F==null){const z="slice_3d_mask failed ...";return o(z,-1,z),0}else if(console.log("--- pre-model done ---"),t){if(i.enableSeqConv)return console.log("------ Mask Cropping & Seq Convoluton ------"),await Be(f,i,r,e,n,l,d,F,o,c,a,s),0;console.log("------ Mask Cropping  -  NO Seq Convoluton ------"),await Ne(r,e,n,l,d,F,i,a,f,c,o,s)}else window.alert("inferenceSubVolumes() is not dead code?")}v++},C)}catch(g){o(g.message,-1,g.message),console.log('If webgl context is lost, try to restore webgl context by visit the link <a href="https://support.biodigital.com/hc/en-us/articles/218322977-How-to-turn-on-WebGL-in-my-browser">here</a>')}}else console.log("--- No pre-model is selected ---"),console.log("------ Run voxel cropping ------"),t?i.enableSeqConv?(console.log("------ Seq Convoluton ------"),await Be(f,i,r,e,n,l,d,null,o,c,a,s)):Ne(r,e,n,l,d,null,i,a,f,c,o,s):window.alert("inferenceSubVolumes() is not dead code?")}async function ao(r=!0){await Qe(),oe().set("DEBUG",!1),oe().set("WEBGL_FORCE_F16_TEXTURES",r),oe().set("WEBGL_DELETE_TEXTURE_THRESHOLD",0),await He(),console.log("tf env() flags :",oe().flags),console.log("tf env() features :",oe().features),console.log("tf env total features: ",Object.keys(oe().features).length),console.log(Te())}async function ro(r,e,n,l,d,t){const i=[];i.startTime=Date.now(),t("Segmentation started",0),performance.now();const a=r.batchSize,f=r.numOfChan;if(isNaN(a)||a!==1){const _="The batch Size for input shape must be 1";return t(_,-1,_),0}if(isNaN(f)||f!==1){const _="The number of channels for input shape must be 1";return t(_,-1,_),0}L().startScope(),console.log("Batch size: ",a),console.log("Num of Channels: ",f);const c=await Ee(r.rootURL+e.path);await ao(!0),i.TF_Backend=Te();const o=c;let s=[];if(s=o.layers[0].batchInputShape,console.log(" Model batch input shape : ",s),s.length!==5){const _="The model input shape must be 5D";return t(_,-1,_),0}let h,p,u;const m=n.dims[1],y=n.dims[2],g=n.dims[3];if(await re(o)){if(console.log("Model Channel Last"),isNaN(s[4])||s[4]!==1){const _="The number of channels for input shape must be 1";return t(_,-1,_),0}h=s[1],p=s[2],u=s[3]}else{if(console.log("Model Channel First"),isNaN(s[1])||s[1]!==1){const _="The number of channels for input shape must be 1";return t(_,-1,_),0}h=s[2],p=s[3],u=s[4]}let b;h===256&&p===256&&u===256?b=!0:b=!1,i.isModelFullVol=b;let I=await De(g,n,l);const R=e.enableTranspose,U=e.enableCrop;b&&(U?await no(c,I,g,y,m,b,e,i,r,d,t,l):(console.log("Cropping Disabled"),R?(I=I.transpose(),console.log("Input transposed")):console.log("Transpose NOT Enabled"),e.enableSeqConv?(console.log("Seq Convoluton Enabled"),window.alert("inferenceFullVolumeSeqCovLayer() is not dead code?")):(console.log("Seq Convoluton Disabled"),window.alert("inferenceFullVolume() is not dead code?"))))}async function we(){return navigator.userAgent.indexOf("OPR/")>-1?"Opera":navigator.userAgent.indexOf("Edg/")>-1?"Edge":navigator.userAgent.indexOf("Falkon/")>-1?"Falkon":navigator.userAgent.indexOf("Chrome/")>-1?"Chrome":navigator.userAgent.indexOf("Firefox/")>-1?"Firefox":navigator.userAgent.indexOf("Safari/")>-1?"Safari":navigator.userAgent.indexOf("MSIE/")>-1||navigator.userAgent.indexOf("rv:")>-1?"IExplorer":"Unknown"}async function so(){return navigator.userAgent.indexOf("OPR/")>-1?parseInt(navigator.userAgent.split("OPR/")[1]):navigator.userAgent.indexOf("Edg/")>-1?parseInt(navigator.userAgent.split("Edg/")[1]):navigator.userAgent.indexOf("Falkon/")>-1?parseInt(navigator.userAgent.split("Falkon/")[1]):navigator.userAgent.indexOf("Chrome/")>-1?parseInt(navigator.userAgent.split("Chrome/")[1]):navigator.userAgent.indexOf("Firefox/")>-1?parseInt(navigator.userAgent.split("Firefox/")[1]):navigator.userAgent.indexOf("Safari/")>-1?parseInt(navigator.userAgent.split("Safari/")[1]):navigator.userAgent.indexOf("MSIE/")>-1||navigator.userAgent.indexOf("rv:")>-1?parseInt(navigator.userAgent.split("MSIE/")[1]):1/0}async function to(){return navigator.userAgent.indexOf("Win")>-1?"Windows":navigator.userAgent.indexOf("Mac")>-1?"MacOS":navigator.userAgent.indexOf("Linux")>-1?"Linux":navigator.userAgent.indexOf("UNIX")>-1?"UNIX":"Unknown"}async function lo(r){return r?(console.log("WebGl2 is enabled"),!0):(console.log(typeof WebGL2RenderingContext!="undefined"?"WebGL2 may be disabled. Please try updating video card drivers":"WebGL2 is not supported"),!1)}async function io(r){let e;if(r&&(e=r.getExtension("WEBGL_debug_renderer_info"),e)){const n=r.getParameter(e.UNMASKED_VENDOR_WEBGL);return n.indexOf("(")>-1&&n.indexOf(")")>-1?n.substring(n.indexOf("(")+1,n.indexOf(")")):n}return null}async function co(r){if(r){const e=r.getExtension("WEBGL_debug_renderer_info");return e?r.getParameter(e.UNMASKED_VENDOR_WEBGL):null}else return null}async function uo(r){if(r){if(we()==="Firefox")return r.getParameter(r.RENDERER);const e=r.getExtension("WEBGL_debug_renderer_info");return e?r.getParameter(e.UNMASKED_RENDERER_WEBGL):null}else return null}async function po(r){let e;if(r){if(we()==="Firefox")return r.getParameter(r.RENDERER);if(e=r.getExtension("WEBGL_debug_renderer_info"),e){let n=r.getParameter(e.UNMASKED_RENDERER_WEBGL);return n.indexOf("(")>-1&&n.indexOf(")")>-1&&n.indexOf("(R)")===-1&&(n=n.substring(n.indexOf("(")+1,n.indexOf(")")),n.split(",").length===3)?n.split(",")[1].trim():n}}return null}async function fo(){return navigator.hardwareConcurrency}async function Fe(){return/Chrome/.test(navigator.userAgent)&&/Google Inc/.test(navigator.vendor)}async function ho(r,e=null){const n=new Date;if(r.isModelFullVol?r.Brainchop_Ver="FullVolume":r.Brainchop_Ver="SubVolumes",r.Total_t=(Date.now()-r.startTime)/1e3,delete r.startTime,r.Date=parseInt(n.getMonth()+1)+"/"+n.getDate()+"/"+n.getFullYear(),r.Browser=await we(),r.Browser_Ver=await so(),r.OS=await to(),r.WebGL2=await lo(e),r.GPU_Vendor=await io(e),r.GPU_Card=await po(e),r.GPU_Vendor_Full=await co(e),r.GPU_Card_Full=await uo(e),r.CPU_Cores=await fo(),r.Which_Brainchop="latest",await Fe()&&(r.Heap_Size_MB=window.performance.memory.totalJSHeapSize/(1024*1024).toFixed(2),r.Used_Heap_MB=window.performance.memory.usedJSHeapSize/(1024*1024).toFixed(2),r.Heap_Limit_MB=window.performance.memory.jsHeapSizeLimit/(1024*1024).toFixed(2)),e){console.log("MAX_TEXTURE_SIZE :",e.getParameter(e.MAX_TEXTURE_SIZE)),console.log("MAX_RENDERBUFFER_SIZE :",e.getParameter(e.MAX_RENDERBUFFER_SIZE));const l=e.getExtension("WEBGL_debug_renderer_info");console.log("VENDOR WEBGL:",e.getParameter(l.UNMASKED_VENDOR_WEBGL)),r.Texture_Size=e.getParameter(e.MAX_TEXTURE_SIZE)}else r.Texture_Size=null;return r}function mo(){return new Worker("/assets/brainchop-webworker.8850e551.js",{type:"module"})}async function go(){dragMode.onchange=async function(){s.opts.dragMode=this.selectedIndex},drawDrop.onchange=async function(){if(s.volumes.length<2){window.alert("No segmentation open (use the Segmentation pull down)"),drawDrop.selectedIndex=-1;return}if(!s.drawBitmap){window.alert("No drawing (hint: use the Draw pull down to select a pen)"),drawDrop.selectedIndex=-1;return}const u=parseInt(this.value);if(u===0){s.drawUndo(),drawDrop.selectedIndex=-1;return}let m=s.volumes[1].img,y=await s.saveImage({filename:"",isSaveDrawing:!0});const g=352,S=y.length;if(u===1)for(let b=0;b<S;b++)y[g+b]>0&&(m[b]=1);if(u===2)for(let b=0;b<S;b++)y[g+b]>0&&(m[b]=0);s.closeDrawing(),s.updateGLVolume(),s.setDrawingEnabled(!1),penDrop.selectedIndex=-1,drawDrop.selectedIndex=-1},penDrop.onchange=async function(){const u=parseInt(this.value);s.setDrawingEnabled(u>=0),u>=0&&s.setPenValue(u&7,u>7)},aboutBtn.onclick=function(){window.alert("Drag and drop NIfTI images. Use pulldown menu to choose brainchop model")},diagnosticsBtn.onclick=function(){if(c.length<1){window.alert("No diagnostic string generated: run a model to create diagnostics");return}navigator.clipboard.writeText(c),window.alert(`Diagnostics copied to clipboard
`+c)},opacitySlider0.oninput=function(){s.setOpacity(0,opacitySlider0.value/255),s.updateGLVolume()},opacitySlider1.oninput=function(){s.setOpacity(1,opacitySlider1.value/255)};async function r(){const u=s.volumes[0];let m=u.dims[1]===256&&u.dims[2]===256&&u.dims[3]===256;if((u.permRAS[0]!==-1||u.permRAS[1]!==3||u.permRAS[2]!==-2)&&(m=!1),m)return;const y=await s.conform(u,!1);await s.removeVolume(s.volumes[0]),await s.addVolume(y)}async function e(){for(;s.volumes.length>1;)await s.removeVolume(s.volumes[1])}modelSelect.onchange=async function(){this.selectedIndex<0&&(modelSelect.selectedIndex=11),await e(),await r();const u=D[this.selectedIndex],m=Ze,y=new URL(window.location.href);if(m.rootURL=y.origin+y.pathname,Boolean(window.location.hostname==="localhost"||window.location.hostname==="[::1]"||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/))&&(m.rootURL=location.protocol+"//"+location.host),workerCheck.checked){if(typeof o!="undefined"){console.log("Unable to start new segmentation: previous call has not completed");return}o=await new mo;const S={datatypeCode:s.volumes[0].hdr.datatypeCode,dims:s.volumes[0].hdr.dims},b={opts:m,modelEntry:u,niftiHeader:S,niftiImage:s.volumes[0].img};o.postMessage(b),o.onmessage=function(I){const R=I.data.cmd;R==="ui"&&(I.data.modalMessage!==""&&(o.terminate(),o=void 0),i(I.data.message,I.data.progressFrac,I.data.modalMessage,I.data.statData)),R==="img"&&(o.terminate(),o=void 0,d(I.data.img,I.data.opts,I.data.modelEntry))}}else ro(m,u,s.volumes[0].hdr,s.volumes[0].img,d,i)},saveImgBtn.onclick=function(){s.volumes[1].saveToDisk("Custom.nii")},saveSceneBtn.onclick=function(){s.saveDocument("brainchop.nvd")},workerCheck.onchange=function(){modelSelect.onchange()},clipCheck.onchange=function(){clipCheck.checked?s.setClipPlane([0,0,90]):s.setClipPlane([2,0,90])};function n(){opacitySlider0.oninput()}async function l(u){return await(await fetch(u)).json()}async function d(u,m,y){e();const g=await s.volumes[0].clone();if(g.zeroImage(),g.hdr.scl_inter=0,g.hdr.scl_slope=1,g.img=new Uint8Array(u),y.colormapPath){const S=await l(y.colormapPath);g.setColormapLabel(S),g.hdr.intent_code=1002}else{let S=m.atlasSelectedColorTable.toLowerCase();s.colormaps().includes(S)||(S="actc"),g.colormap=S}g.opacity=opacitySlider1.value/255,await s.addVolume(g)}async function t(u){(typeof u=="string"||u instanceof String)&&(u=function(y){const g=JSON.parse(y),S=[];for(const b in g)S[b]=g[b];return S}(u)),u=await ho(u,s.gl),c=`:: Diagnostics can help resolve issues https://github.com/neuroneural/brainchop/issues ::
`;for(const m in u)c+=m+": "+u[m]+`
`}function i(u="",m=-1,y="",g=[]){u!==""&&(console.log(u),document.getElementById("location").innerHTML=u),isNaN(m)?(memstatus.style.color="red",memstatus.innerHTML="Memory Issue"):m>=0&&(modelProgress.value=m*modelProgress.max),y!==""&&window.alert(y),Object.keys(g).length>0&&t(g)}function a(u){document.getElementById("location").innerHTML="&nbsp;&nbsp;"+u.string}const f={backColor:[.4,.4,.4,1],show3Dcrosshair:!0,onLocationChange:a};let c="",o;const s=new Xe(f);s.attachToCanvas(gl1),s.opts.dragMode=s.dragModes.pan,s.opts.multiplanarForceRender=!0,s.opts.yoke3Dto2DZoom=!0,s.opts.crosshairGap=11,s.setInterpolation(!0),await s.loadVolumes([{url:"./t1_crop.nii.gz"}]);for(let u=0;u<D.length;u++){const m=document.createElement("option");m.text=D[u].modelName,m.value=D[u].id.toString(),modelSelect.appendChild(m)}s.onImageLoaded=n,modelSelect.selectedIndex=-1,drawDrop.selectedIndex=-1,workerCheck.checked=await Fe();const p=new URLSearchParams(window.location.search).get("model");p&&(modelSelect.selectedIndex=Number(p),modelSelect.onchange())}go();
